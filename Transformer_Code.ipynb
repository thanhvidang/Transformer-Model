{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0pQsvfavG5F-"
      },
      "outputs": [],
      "source": [
        "%pip install tensorflow==2.12.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvYG32kUKX1E"
      },
      "source": [
        "**LOAD DATA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KlPWQAKBG837",
        "outputId": "f00f7038-f68d-484f-df0e-88f2365d291d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from http://storage.googleapis.com/download.tensorflow.org/data/fra-eng.zip\n",
            "3423204/3423204 [==============================] - 0s 0us/step\n",
            "/root/.keras/datasets/fra.txt\n"
          ]
        }
      ],
      "source": [
        "import pathlib\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# download dataset provided by Anki: https://www.manythings.org/anki/\n",
        "text_file = tf.keras.utils.get_file(\n",
        "    fname=\"fra-eng.zip\",\n",
        "    origin=\"http://storage.googleapis.com/download.tensorflow.org/data/fra-eng.zip\",\n",
        "    extract=True,\n",
        ")\n",
        "# show where the file is located now\n",
        "text_file = pathlib.Path(text_file).parent / \"fra.txt\"\n",
        "print(text_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMA2cpU0KbRz"
      },
      "source": [
        "**TEXT NORMALIZATION**\n",
        "> Normalization: covert into NFKC - compatibility and compositional normal form\n",
        "\n",
        "\n",
        "> Tokenization: separating words into tokens but not punctuation\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HBjSOX9rHBTq",
        "outputId": "c4370712-b38d-4c5b-a672-ed7bad216235"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(\"i'll wait in the gym .\", \"[start] j'attendrai à la salle de sport . [end]\")\n",
            "('i cannot look at this picture without thinking of my dead mother .', '[start] je ne peux regarder cette photo sans penser à ma défunte mère . [end]')\n",
            "('my nose itches .', '[start] le nez me gratte . [end]')\n",
            "('are you a policeman ?', '[start] êtes-vous un agent de police  ?  [end]')\n",
            "('her father works at the bank .', '[start] son père travaille à la banque . [end]')\n"
          ]
        }
      ],
      "source": [
        "import pathlib\n",
        "import pickle\n",
        "import random\n",
        "import re\n",
        "import unicodedata\n",
        "\n",
        "# download dataset provided by Anki: https://www.manythings.org/anki/\n",
        "text_file = tf.keras.utils.get_file(\n",
        "    fname=\"fra-eng.zip\",\n",
        "    origin=\"http://storage.googleapis.com/download.tensorflow.org/data/fra-eng.zip\",\n",
        "    extract=True,\n",
        ")\n",
        "text_file = pathlib.Path(text_file).parent / \"fra.txt\"\n",
        "\n",
        "def normalize(line):\n",
        "    \"\"\"Normalize a line of text and split into two at the tab character\"\"\"\n",
        "    line = unicodedata.normalize(\"NFKC\", line.strip().lower())\n",
        "    line = re.sub(r\"^([^ \\w])(?!\\s)\", r\"\\1 \", line)\n",
        "    line = re.sub(r\"(\\s[^ \\w])(?!\\s)\", r\"\\1 \", line)\n",
        "    line = re.sub(r\"(?!\\s)([^ \\w])$\", r\" \\1\", line)\n",
        "    line = re.sub(r\"(?!\\s)([^ \\w]\\s)\", r\" \\1\", line)\n",
        "    eng, fra = line.split(\"\\t\")\n",
        "    fra = \"[start] \" + fra + \" [end]\"\n",
        "    return eng, fra\n",
        "\n",
        "# normalize each line and separate into English and French\n",
        "with open(text_file) as fp:\n",
        "    text_pairs = [normalize(line) for line in fp]\n",
        "\n",
        "# print some samples\n",
        "for _ in range(5):\n",
        "    print(random.choice(text_pairs))\n",
        "\n",
        "with open(\"text_pairs.pickle\", \"wb\") as fp:\n",
        "    pickle.dump(text_pairs, fp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ZvD6BnfSlhM",
        "outputId": "f6824dcb-48d9-4ae9-b7f5-9611b5a471b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total English tokens: 14969\n",
            "Total French tokens: 29219\n",
            "Max English length: 51\n",
            "Max French length: 60\n",
            "167130 total pairs\n"
          ]
        }
      ],
      "source": [
        "#Getting statistics of dataset\n",
        "\n",
        "with open(\"text_pairs.pickle\", \"rb\") as fp:\n",
        "    text_pairs = pickle.load(fp)\n",
        "\n",
        "# count tokens\n",
        "eng_tokens, fra_tokens = set(), set()\n",
        "eng_maxlen, fra_maxlen = 0, 0\n",
        "for eng, fra in text_pairs:\n",
        "    eng_tok, fra_tok = eng.split(), fra.split()\n",
        "    eng_maxlen = max(eng_maxlen, len(eng_tok))\n",
        "    fra_maxlen = max(fra_maxlen, len(fra_tok))\n",
        "    eng_tokens.update(eng_tok)\n",
        "    fra_tokens.update(fra_tok)\n",
        "print(f\"Total English tokens: {len(eng_tokens)}\")\n",
        "print(f\"Total French tokens: {len(fra_tokens)}\")\n",
        "print(f\"Max English length: {eng_maxlen}\")\n",
        "print(f\"Max French length: {fra_maxlen}\")\n",
        "print(f\"{len(text_pairs)} total pairs\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6vrU14DSyNb"
      },
      "outputs": [],
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# with open(\"text_pairs.pickle\", \"rb\") as fp:\n",
        "#     text_pairs = pickle.load(fp)\n",
        "\n",
        "# # histogram of sentence length in tokens\n",
        "# en_lengths = [len(eng.split()) for eng, fra in text_pairs]\n",
        "# fr_lengths = [len(fra.split()) for eng, fra in text_pairs]\n",
        "\n",
        "# plt.hist(en_lengths, label=\"en\", color=\"red\", alpha=0.33)\n",
        "# plt.hist(fr_lengths, label=\"fr\", color=\"blue\", alpha=0.33)\n",
        "# plt.yscale(\"log\")     # sentence length fits Benford\"s law\n",
        "# plt.ylim(plt.ylim())  # make y-axis consistent for both plots\n",
        "# plt.plot([max(en_lengths), max(en_lengths)], plt.ylim(), color=\"red\")\n",
        "# plt.plot([max(fr_lengths), max(fr_lengths)], plt.ylim(), color=\"blue\")\n",
        "# plt.legend()\n",
        "# plt.title(\"Examples count vs Token length\")\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-PBUIkBO3RH"
      },
      "source": [
        "**TEXT VECTORIZATION**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1zOOnSzdS4vx"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import TextVectorization\n",
        "\n",
        "# Load normalized sentence pairs\n",
        "with open(\"text_pairs.pickle\", \"rb\") as fp:\n",
        "    text_pairs = pickle.load(fp)\n",
        "\n",
        "# train-test-val split of randomized sentence pairs\n",
        "random.shuffle(text_pairs)\n",
        "n_val = int(0.15*len(text_pairs))\n",
        "n_train = len(text_pairs) - 2*n_val\n",
        "train_pairs = text_pairs[:n_train]\n",
        "val_pairs = text_pairs[n_train:n_train+n_val]\n",
        "test_pairs = text_pairs[n_train+n_val:]\n",
        "\n",
        "# Parameter determined after analyzing the input data\n",
        "vocab_size_en = 10000\n",
        "vocab_size_fr = 20000\n",
        "seq_length = 20\n",
        "\n",
        "# Create vectorizer\n",
        "eng_vectorizer = TextVectorization(\n",
        "    max_tokens=vocab_size_en,\n",
        "    standardize=None,\n",
        "    split=\"whitespace\",\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=seq_length,\n",
        ")\n",
        "fra_vectorizer = TextVectorization(\n",
        "    max_tokens=vocab_size_fr,\n",
        "    standardize=None,\n",
        "    split=\"whitespace\",\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=seq_length + 1\n",
        ")\n",
        "\n",
        "# train the vectorization layer using training dataset\n",
        "train_eng_texts = [pair[0] for pair in train_pairs]\n",
        "train_fra_texts = [pair[1] for pair in train_pairs]\n",
        "eng_vectorizer.adapt(train_eng_texts)\n",
        "fra_vectorizer.adapt(train_fra_texts)\n",
        "\n",
        "# save for subsequent steps\n",
        "with open(\"vectorize.pickle\", \"wb\") as fp:\n",
        "    data = {\n",
        "        \"train\": train_pairs,\n",
        "        \"val\":   val_pairs,\n",
        "        \"test\":  test_pairs,\n",
        "        \"engvec_config\":  eng_vectorizer.get_config(),\n",
        "        \"engvec_weights\": eng_vectorizer.get_weights(),\n",
        "        \"fravec_config\":  fra_vectorizer.get_config(),\n",
        "        \"fravec_weights\": fra_vectorizer.get_weights(),\n",
        "    }\n",
        "    pickle.dump(data, fp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PcRomqyfTC1x",
        "outputId": "7721fb78-a746-4f77-f998-2c08383d99f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "inputs[\"encoder_inputs\"].shape: (64, 20)\n",
            "inputs[\"encoder_inputs\"][0]: [ 10 670  18   5  45   2   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0]\n",
            "inputs[\"decoder_inputs\"].shape: (64, 20)\n",
            "inputs[\"decoder_inputs\"][0]: [   2   16  108 1114    6  205    4    3    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0]\n",
            "targets.shape: (64, 20)\n",
            "targets[0]: [  16  108 1114    6  205    4    3    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0]\n"
          ]
        }
      ],
      "source": [
        "# load text data and vectorizer weights\n",
        "with open(\"vectorize.pickle\", \"rb\") as fp:\n",
        "    data = pickle.load(fp)\n",
        "\n",
        "train_pairs = data[\"train\"]\n",
        "val_pairs = data[\"val\"]\n",
        "test_pairs = data[\"test\"]   # not used\n",
        "\n",
        "eng_vectorizer = TextVectorization.from_config(data[\"engvec_config\"])\n",
        "eng_vectorizer.set_weights(data[\"engvec_weights\"])\n",
        "fra_vectorizer = TextVectorization.from_config(data[\"fravec_config\"])\n",
        "fra_vectorizer.set_weights(data[\"fravec_weights\"])\n",
        "\n",
        "# set up Dataset object\n",
        "def format_dataset(eng, fra):\n",
        "    eng = eng_vectorizer(eng)\n",
        "    fra = fra_vectorizer(fra)\n",
        "    source = {\"encoder_inputs\": eng,\n",
        "              \"decoder_inputs\": fra[:, :-1]}\n",
        "    target = fra[:, 1:]\n",
        "    return (source, target)\n",
        "\n",
        "def make_dataset(pairs, batch_size=64):\n",
        "    # aggregate sentences using zip(*pairs)\n",
        "    eng_texts, fra_texts = zip(*pairs)\n",
        "    # convert them into list, and then create tensors\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((list(eng_texts), list(fra_texts)))\n",
        "    return dataset.shuffle(2048) \\\n",
        "                  .batch(batch_size).map(format_dataset) \\\n",
        "                  .prefetch(16).cache()\n",
        "\n",
        "train_ds = make_dataset(train_pairs)\n",
        "val_ds = make_dataset(val_pairs)\n",
        "\n",
        "# test the dataset\n",
        "for inputs, targets in train_ds.take(1):\n",
        "    print(f'inputs[\"encoder_inputs\"].shape: {inputs[\"encoder_inputs\"].shape}')\n",
        "    print(f'inputs[\"encoder_inputs\"][0]: {inputs[\"encoder_inputs\"][0]}')\n",
        "    print(f'inputs[\"decoder_inputs\"].shape: {inputs[\"decoder_inputs\"].shape}')\n",
        "    print(f'inputs[\"decoder_inputs\"][0]: {inputs[\"decoder_inputs\"][0]}')\n",
        "    print(f\"targets.shape: {targets.shape}\")\n",
        "    print(f\"targets[0]: {targets[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKo-aR96kfeV"
      },
      "source": [
        "**POSITIONAL ENCODING MATRIX**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KV_ztGOtUQGj"
      },
      "outputs": [],
      "source": [
        "#POSITIONAL ENCODING MATRIX\n",
        "def pos_enc_matrix(L, d, n=10000):\n",
        "    assert d % 2 == 0, \"Output dimension needs to be an even integer\"\n",
        "    d2 = d//2\n",
        "    P = np.zeros((L, d))\n",
        "    k = np.arange(L).reshape(-1, 1)     # L-column vector\n",
        "    i = np.arange(d2).reshape(1, -1)    # d-row vector\n",
        "    denom = np.power(n, -i/d2)          # n**(-2*i/d)\n",
        "    args = k * denom                    # (L,d) matrix\n",
        "    P[:, ::2] = np.sin(args)\n",
        "    P[:, 1::2] = np.cos(args)\n",
        "    return P\n",
        "\n",
        "#POSITIONAL EMBEDDING LAYER\n",
        "class PositionalEmbedding(tf.keras.layers.Layer):\n",
        "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.sequence_length = sequence_length\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim     # d_model in paper\n",
        "        # token embedding layer: Convert integer token to D-dim float vector\n",
        "        self.token_embeddings = tf.keras.layers.Embedding(\n",
        "            input_dim=vocab_size, output_dim=embed_dim, mask_zero=True\n",
        "        )\n",
        "        # positional embedding layer: a matrix of hard-coded sine values\n",
        "        matrix = pos_enc_matrix(sequence_length, embed_dim)\n",
        "        self.position_embeddings = tf.constant(matrix, dtype=\"float32\")\n",
        "\n",
        "    def call(self, inputs):\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        return embedded_tokens + self.position_embeddings\n",
        "\n",
        "    def compute_mask(self, *args, **kwargs):\n",
        "        return self.token_embeddings.compute_mask(*args, **kwargs)\n",
        "\n",
        "    def get_config(self):\n",
        "        # to make save and load a model using custom layer possible\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"sequence_length\": self.sequence_length,\n",
        "            \"vocab_size\": self.vocab_size,\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "        })\n",
        "        return config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bg12WWLWUUk_",
        "outputId": "3916fb4e-675c-446c-fa95-c3349aa6aee5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[   3   31    5 ...    0    0    0]\n",
            " [  12 4977  338 ...    0    0    0]\n",
            " [  27   41    4 ...    0    0    0]\n",
            " ...\n",
            " [  93   13    4 ...    0    0    0]\n",
            " [   4   38   45 ...    0    0    0]\n",
            " [ 140  118   11 ...    0    0    0]], shape=(64, 20), dtype=int64)\n",
            "(64, 20, 512)\n",
            "tf.Tensor(\n",
            "[[ True  True  True ... False False False]\n",
            " [ True  True  True ... False False False]\n",
            " [ True  True  True ... False False False]\n",
            " ...\n",
            " [ True  True  True ... False False False]\n",
            " [ True  True  True ... False False False]\n",
            " [ True  True  True ... False False False]], shape=(64, 20), dtype=bool)\n"
          ]
        }
      ],
      "source": [
        "vocab_size_en = 10000\n",
        "seq_length = 20\n",
        "\n",
        "# test the dataset\n",
        "for inputs, targets in train_ds.take(1):\n",
        "    print(inputs[\"encoder_inputs\"])\n",
        "    embed_en = PositionalEmbedding(seq_length, vocab_size_en, embed_dim=512)\n",
        "    en_emb = embed_en(inputs[\"encoder_inputs\"])\n",
        "    print(en_emb.shape)\n",
        "    print(en_emb._keras_mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ql7TG27ctipb"
      },
      "source": [
        "**TRANSFORMER BLOCK**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHFzVkjo-NRe"
      },
      "source": [
        "*Self-Attention Model*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L7vHXxSUUYeQ"
      },
      "outputs": [],
      "source": [
        "\"\"\"The attention mechanism is applied to the inputs tensor, where the query, value, and key are all set to the inputs tensor itself.\n",
        "The purpose is to capture dependencies within the input sequence and attend to different parts of the sequence based on their relevance to each other.\n",
        "(This function assumes its input is the output from positional encoding layer.)\"\"\"\n",
        "\n",
        "def self_attention(input_shape, prefix=\"att\", mask=False, **kwargs):\n",
        "    # create layers\n",
        "    # Define an input layer with the specified input_shape and name.\n",
        "    inputs = tf.keras.layers.Input(shape=input_shape, dtype='float32',\n",
        "                                   name=f\"{prefix}_in1\")\n",
        "\n",
        "    # Create a MultiHeadAttention layer with the given keyword arguments and name.\n",
        "    attention = tf.keras.layers.MultiHeadAttention(name=f\"{prefix}_attn1\", **kwargs)\n",
        "\n",
        "    # Create a LayerNormalization layer\n",
        "    norm = tf.keras.layers.LayerNormalization(name=f\"{prefix}_norm1\")\n",
        "\n",
        "    # Create an Add layer\n",
        "    add = tf.keras.layers.Add(name=f\"{prefix}_add1\")\n",
        "\n",
        "    # functional API to connect input to output\n",
        "    # Apply the attention layer to the inputs using the same inputs as query, value, and key.\n",
        "    # If mask is True, a causal mask is applied during the attention calculation.\n",
        "    attout = attention(query=inputs, value=inputs, key=inputs,\n",
        "                       use_causal_mask=mask)\n",
        "\n",
        "    # Add the inputs and attout using the Add layer, and then apply layer normalization using the norm layer to the sum.\n",
        "    outputs = norm(add([inputs, attout]))\n",
        "\n",
        "    # Create a Keras model with the inputs and outputs defined.\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=outputs, name=f\"{prefix}_att\")\n",
        "    return model\n",
        "\n",
        "seq_length = 20\n",
        "key_dim = 128\n",
        "num_heads = 8\n",
        "\n",
        "model = self_attention(input_shape=(seq_length, key_dim),\n",
        "                       num_heads=num_heads, key_dim=key_dim)\n",
        "\n",
        "# Visualization of self_attention function\n",
        "# tf.keras.utils.plot_model(model, \"self-attention.png\",\n",
        "#                           show_shapes=True, show_dtype=True, show_layer_names=True,\n",
        "#                           rankdir='BT', show_layer_activations=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24c9h-P2-S9F"
      },
      "source": [
        "*Cross-Attention Model*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1GMEjXEAnU3g"
      },
      "outputs": [],
      "source": [
        "\"\"\"The attention mechanism is applied between two tensors: inputs (eng) and context (fre).\n",
        "The inputs tensor serves as the query, while the context tensor serves as both the value and the key.\n",
        "The purpose is to attend to relevant information in the context tensor based on the query in the inputs tensor.\n",
        "This allows the model to consider information from a different context, such as attending to relevant information in an encoder-decoder architecture.\n",
        "In this case, this function allows the model to attend to the relevant parts of the English input sentence while generating each word of the French context sentence during the translation process.\n",
        "\"\"\"\n",
        "\n",
        "def cross_attention(input_shape, context_shape, prefix=\"att\", **kwargs):\n",
        "    # create layers\n",
        "    # Define an input layer for the context (french).\n",
        "    context = tf.keras.layers.Input(shape=context_shape, dtype='float32',\n",
        "                                    name=f\"{prefix}_ctx2\")\n",
        "\n",
        "    # Define an input layer for the inputs (english)\n",
        "    inputs = tf.keras.layers.Input(shape=input_shape, dtype='float32',\n",
        "                                   name=f\"{prefix}_in2\")\n",
        "\n",
        "    # Create a MultiHeadAttention layer.\n",
        "    attention = tf.keras.layers.MultiHeadAttention(name=f\"{prefix}_attn2\", **kwargs)\n",
        "\n",
        "    # Create a LayerNormalization layer.\n",
        "    norm = tf.keras.layers.LayerNormalization(name=f\"{prefix}_norm2\")\n",
        "\n",
        "    # Create an Add layer.\n",
        "    add = tf.keras.layers.Add(name=f\"{prefix}_add2\")\n",
        "\n",
        "    # functional API to connect input to output\n",
        "    # Apply the attention layer to the inputs using the context as the value and key, and inputs as the query.\n",
        "    attout = attention(query=inputs, value=context, key=context)\n",
        "\n",
        "    # Add the attout and inputs using the Add layer, and then apply layer normalization using the norm layer to the sum.\n",
        "    outputs = norm(add([attout, inputs]))\n",
        "    # create model and return\n",
        "    model = tf.keras.Model(inputs=[(context, inputs)], outputs=outputs,\n",
        "                           name=f\"{prefix}_cross\")\n",
        "    return model\n",
        "\n",
        "seq_length = 20\n",
        "key_dim = 128\n",
        "num_heads = 8\n",
        "\n",
        "model = cross_attention(input_shape=(seq_length, key_dim),\n",
        "                        context_shape=(seq_length, key_dim),\n",
        "                        num_heads=num_heads, key_dim=key_dim)\n",
        "# tf.keras.utils.plot_model(model, \"cross-attention.png\",\n",
        "#                           show_shapes=True, show_dtype=True, show_layer_names=True,\n",
        "#                           rankdir='BT', show_layer_activations=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzEkRf95-Wo1"
      },
      "source": [
        "*Feed-Forward Model*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qReXv-UPpfwg"
      },
      "outputs": [],
      "source": [
        "def feed_forward(input_shape, model_dim, ff_dim, dropout=0.1, prefix=\"ff\"):\n",
        "    # create layers\n",
        "    inputs = tf.keras.layers.Input(shape=input_shape, dtype='float32',\n",
        "                                   name=f\"{prefix}_in3\")\n",
        "    dense1 = tf.keras.layers.Dense(ff_dim, name=f\"{prefix}_ff1\", activation=\"relu\")\n",
        "    dense2 = tf.keras.layers.Dense(model_dim, name=f\"{prefix}_ff2\")\n",
        "    drop = tf.keras.layers.Dropout(dropout, name=f\"{prefix}_drop\")\n",
        "    add = tf.keras.layers.Add(name=f\"{prefix}_add3\")\n",
        "    # functional API to connect input to output\n",
        "    ffout = drop(dense2(dense1(inputs)))\n",
        "    norm = tf.keras.layers.LayerNormalization(name=f\"{prefix}_norm3\")\n",
        "    outputs = norm(add([inputs, ffout]))\n",
        "    # create model and return\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=outputs, name=f\"{prefix}_ff\")\n",
        "    return model\n",
        "\n",
        "seq_length = 20\n",
        "key_dim = 128\n",
        "ff_dim = 512\n",
        "\n",
        "model = feed_forward(input_shape=(seq_length, key_dim),\n",
        "                     model_dim=key_dim, ff_dim=ff_dim)\n",
        "# tf.keras.utils.plot_model(model, \"feedforward.png\",\n",
        "#                           show_shapes=True, show_dtype=True, show_layer_names=True,\n",
        "#                           rankdir='BT', show_layer_activations=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBYd9x9j-cTt"
      },
      "source": [
        "*Encoder Block*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hBuKxNC2w92h"
      },
      "outputs": [],
      "source": [
        "def encoder(input_shape, key_dim, ff_dim, dropout=0.1, prefix=\"enc\", **kwargs):\n",
        "    model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Input(shape=input_shape, dtype='float32', name=f\"{prefix}_in0\"),\n",
        "        self_attention(input_shape, prefix=prefix, key_dim=key_dim, mask=False, **kwargs),\n",
        "        feed_forward(input_shape, key_dim, ff_dim, dropout, prefix),\n",
        "    ], name=prefix)\n",
        "    return model\n",
        "\n",
        "\n",
        "seq_length = 20\n",
        "key_dim = 128\n",
        "ff_dim = 512\n",
        "num_heads = 8\n",
        "\n",
        "model = encoder(input_shape=(seq_length, key_dim), key_dim=key_dim, ff_dim=ff_dim,\n",
        "                num_heads=num_heads)\n",
        "# tf.keras.utils.plot_model(model, \"encoder.png\",\n",
        "#                           show_shapes=True, show_dtype=True, show_layer_names=True,\n",
        "#                           rankdir='BT', show_layer_activations=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kJLXF1A-fQV"
      },
      "source": [
        "*Decoder Block*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NOBxaRZ4xKsV"
      },
      "outputs": [],
      "source": [
        "def decoder(input_shape, key_dim, ff_dim, dropout=0.1, prefix=\"dec\", **kwargs):\n",
        "    inputs = tf.keras.layers.Input(shape=input_shape, dtype='float32',\n",
        "                                   name=f\"{prefix}_in0\")\n",
        "    context = tf.keras.layers.Input(shape=input_shape, dtype='float32',\n",
        "                                    name=f\"{prefix}_ctx0\")\n",
        "    attmodel = self_attention(input_shape, key_dim=key_dim, mask=True,\n",
        "                              prefix=prefix, **kwargs)\n",
        "    crossmodel = cross_attention(input_shape, input_shape, key_dim=key_dim,\n",
        "                                 prefix=prefix, **kwargs)\n",
        "    ffmodel = feed_forward(input_shape, key_dim, ff_dim, dropout, prefix)\n",
        "    x = attmodel(inputs)\n",
        "    x = crossmodel([(context, x)])\n",
        "    output = ffmodel(x)\n",
        "    model = tf.keras.Model(inputs=[(inputs, context)], outputs=output, name=prefix)\n",
        "    return model\n",
        "\n",
        "\n",
        "seq_length = 20\n",
        "key_dim = 128\n",
        "ff_dim = 512\n",
        "num_heads = 8\n",
        "\n",
        "model = decoder(input_shape=(seq_length, key_dim), key_dim=key_dim, ff_dim=ff_dim,\n",
        "                num_heads=num_heads)\n",
        "# tf.keras.utils.plot_model(model, \"decoder.png\",\n",
        "#                           show_shapes=True, show_dtype=True, show_layer_names=True,\n",
        "#                           rankdir='BT', show_layer_activations=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maF57874BBso"
      },
      "source": [
        "*Transfomer Block*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ibgdqkL3xQEH"
      },
      "outputs": [],
      "source": [
        "def transformer(num_layers, num_heads, seq_len, key_dim, ff_dim, vocab_size_src,\n",
        "                vocab_size_tgt, dropout=0.1, name=\"transformer\"):\n",
        "    embed_shape = (seq_len, key_dim)  # output shape of the positional embedding layer\n",
        "\n",
        "    # set up layers\n",
        "    # Define an input layer for the encoder inputs.\n",
        "    input_enc = tf.keras.layers.Input(shape=(seq_len,), dtype=\"int32\",\n",
        "                                      name=\"encoder_inputs\")\n",
        "\n",
        "    # Define an input layer for the decoder inputs\n",
        "    input_dec = tf.keras.layers.Input(shape=(seq_len,), dtype=\"int32\",\n",
        "                                      name=\"decoder_inputs\")\n",
        "\n",
        "    # Create a positional embedding layer for the encoder inputs with the specified sequence length, source vocabulary size, and key dimension.\n",
        "    embed_enc = PositionalEmbedding(seq_len, vocab_size_src, key_dim, name=\"embed_enc\")\n",
        "\n",
        "    # Create a positional embedding layer for the decoder inputs\n",
        "    embed_dec = PositionalEmbedding(seq_len, vocab_size_tgt, key_dim, name=\"embed_dec\")\n",
        "\n",
        "    # Create a list of encoder layers with the.\n",
        "    # Each encoder layer is created using the 'encoder' function with a unique prefix name.\n",
        "    encoders = [encoder(input_shape=embed_shape, key_dim=key_dim,\n",
        "                        ff_dim=ff_dim, dropout=dropout, prefix=f\"enc{i}\",\n",
        "                        num_heads=num_heads)\n",
        "                for i in range(num_layers)]\n",
        "\n",
        "    # Create a list of decoder layers\n",
        "    # Each decoder layer is created using the 'decoder' function with a unique prefix name.\n",
        "    decoders = [decoder(input_shape=embed_shape, key_dim=key_dim,\n",
        "                        ff_dim=ff_dim, dropout=dropout, prefix=f\"dec{i}\",\n",
        "                        num_heads=num_heads)\n",
        "                for i in range(num_layers)]\n",
        "\n",
        "    # Create a Dense layer with the specified target vocabulary size and name.\n",
        "    final = tf.keras.layers.Dense(vocab_size_tgt, name=\"linear\")\n",
        "\n",
        "    # build output\n",
        "    # Apply the encoder positional embedding to the encoder inputs.\n",
        "    x1 = embed_enc(input_enc)\n",
        "\n",
        "    # Apply the decoder positional embedding to the decoder inputs.\n",
        "    x2 = embed_dec(input_dec)\n",
        "\n",
        "\n",
        "\"\"\"the for loop is necessary to ensure the sequential application of multiple layers,\n",
        "allowing the model to benefit from the depth and complexity of the transformer architecture.\"\"\"\n",
        "    # Pass the encoded inputs through each encoder layer in a loop.\n",
        "    for layer in encoders:\n",
        "        x1 = layer(x1)\n",
        "\n",
        "    # Pass the decoded inputs and the encoded inputs through each decoder layer in a loop.\n",
        "    for layer in decoders:\n",
        "        x2 = layer([x2, x1])\n",
        "\n",
        "    # Apply the final Dense layer to the output of the last decoder layer.\n",
        "    output = final(x2)\n",
        "\n",
        "    # This try-except block removes the _keras_mask attribute from the output tensor, if it exists.\n",
        "    # This is necessary for compatibility with certain versions of TensorFlow.\n",
        "    try:\n",
        "        del output._keras_mask\n",
        "    except AttributeError:\n",
        "        pass\n",
        "    model = tf.keras.Model(inputs=[input_enc, input_dec], outputs=output, name=name)\n",
        "    return model\n",
        "\n",
        "\n",
        "seq_len = 20\n",
        "num_layers = 4\n",
        "num_heads = 8\n",
        "key_dim = 128\n",
        "ff_dim = 512\n",
        "dropout = 0.1\n",
        "vocab_size_en = 10000\n",
        "vocab_size_fr = 20000\n",
        "model = transformer(num_layers, num_heads, seq_len, key_dim, ff_dim,\n",
        "                    vocab_size_en, vocab_size_fr, dropout)\n",
        "# tf.keras.utils.plot_model(model, \"transformer.png\",\n",
        "#                           show_shapes=True, show_dtype=True, show_layer_names=True,\n",
        "#                           rankdir='BT', show_layer_activations=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Ic0AXmaBJaA"
      },
      "source": [
        "*Adam the Optimizer*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJ6P9y9l_gpj"
      },
      "outputs": [],
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    \"Custom learning rate for Adam optimizer\"\n",
        "    def __init__(self, key_dim, warmup_steps=4000):\n",
        "        super().__init__()\n",
        "        self.key_dim = key_dim\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.d = tf.cast(self.key_dim, tf.float32)\n",
        "\n",
        "    def __call__(self, step):\n",
        "        step = tf.cast(step, dtype=tf.float32)\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps ** -1.5)\n",
        "        return tf.math.rsqrt(self.d) * tf.math.minimum(arg1, arg2)\n",
        "\n",
        "    def get_config(self):\n",
        "        # to make save and load a model using custom layer possible0\n",
        "        config = {\n",
        "            \"key_dim\": self.key_dim,\n",
        "            \"warmup_steps\": self.warmup_steps,\n",
        "        }\n",
        "        return config\n",
        "\n",
        "key_dim = 128\n",
        "lr = CustomSchedule(key_dim)\n",
        "optimizer = tf.keras.optimizers.Adam(lr, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
        "\n",
        "# plt.plot(lr(tf.range(50000, dtype=tf.float32)))\n",
        "# plt.ylabel('Learning Rate')\n",
        "# plt.xlabel('Train Step')\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGc94WEyCctB"
      },
      "source": [
        "*Loss and Accuracy Metric*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W3JnIPcY_k2i"
      },
      "outputs": [],
      "source": [
        "def masked_loss(label, pred):\n",
        "    mask = label != 0\n",
        "\n",
        "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "        from_logits=True, reduction='none')\n",
        "    loss = loss_object(label, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss.dtype)\n",
        "    loss *= mask\n",
        "    loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
        "    return loss\n",
        "\n",
        "\n",
        "def masked_accuracy(label, pred):\n",
        "    pred = tf.argmax(pred, axis=2)\n",
        "    label = tf.cast(label, pred.dtype)\n",
        "    match = label == pred\n",
        "\n",
        "    mask = label != 0\n",
        "\n",
        "    match = match & mask\n",
        "\n",
        "    match = tf.cast(match, dtype=tf.float32)\n",
        "    mask = tf.cast(mask, dtype=tf.float32)\n",
        "    return tf.reduce_sum(match)/tf.reduce_sum(mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gP8LOVjW_nBB"
      },
      "outputs": [],
      "source": [
        "vocab_size_en = 10000\n",
        "vocab_size_fr = 20000\n",
        "seq_len = 20\n",
        "num_layers = 4\n",
        "num_heads = 8\n",
        "key_dim = 128\n",
        "ff_dim = 512\n",
        "dropout = 0.1\n",
        "model = transformer(num_layers, num_heads, seq_len, key_dim, ff_dim,\n",
        "                    vocab_size_en, vocab_size_fr, dropout)\n",
        "lr = CustomSchedule(key_dim)\n",
        "optimizer = tf.keras.optimizers.Adam(lr, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
        "model.compile(loss=masked_loss, optimizer=optimizer, metrics=[masked_accuracy])\n",
        "# model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrieKKuBBzjw"
      },
      "source": [
        "**TRAINING THE MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "pK03mH_B_-ok",
        "outputId": "5a7b1903-5270-459d-bfa9-c41045124252"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1828/1828 [==============================] - 6493s 4s/step - loss: 5.2070 - masked_accuracy: 0.3014 - val_loss: 3.1783 - val_masked_accuracy: 0.4691\n",
            "Epoch 2/5\n",
            "1828/1828 [==============================] - 6557s 4s/step - loss: 2.7125 - masked_accuracy: 0.5428 - val_loss: 2.2494 - val_masked_accuracy: 0.6044\n",
            "Epoch 3/5\n",
            "1828/1828 [==============================] - 6528s 4s/step - loss: 2.1160 - masked_accuracy: 0.6286 - val_loss: 1.8709 - val_masked_accuracy: 0.6619\n",
            "Epoch 4/5\n",
            "1197/1828 [==================>...........] - ETA: 33:11 - loss: 1.8041 - masked_accuracy: 0.6750"
          ]
        }
      ],
      "source": [
        "# Create and train the model\n",
        "seq_len = 20\n",
        "num_layers = 4\n",
        "num_heads = 8\n",
        "key_dim = 128\n",
        "ff_dim = 512\n",
        "dropout = 0.1\n",
        "vocab_size_en = 10000\n",
        "vocab_size_fr = 20000\n",
        "model = transformer(num_layers, num_heads, seq_len, key_dim, ff_dim,\n",
        "                    vocab_size_en, vocab_size_fr, dropout)\n",
        "lr = CustomSchedule(key_dim)\n",
        "optimizer = tf.keras.optimizers.Adam(lr, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
        "model.compile(loss=masked_loss, optimizer=optimizer, metrics=[masked_accuracy])\n",
        "epochs = 5\n",
        "history = model.fit(train_ds, epochs=epochs, validation_data=val_ds)\n",
        "\n",
        "# Save the trained model\n",
        "model.save(\"eng-fra-transformer.h5\")\n",
        "\n",
        "# # Plot the loss and accuracy history\n",
        "# fig, axs = plt.subplots(2, figsize=(6, 8), sharex=True)\n",
        "# fig.suptitle('Traininig history')\n",
        "# x = list(range(1, epochs+1))\n",
        "# axs[0].plot(x, history.history[\"loss\"], alpha=0.5, label=\"loss\")\n",
        "# axs[0].plot(x, history.history[\"val_loss\"], alpha=0.5, label=\"val_loss\")\n",
        "# axs[0].set_ylabel(\"Loss\")\n",
        "# axs[0].legend(loc=\"upper right\")\n",
        "# axs[1].plot(x, history.history[\"masked_accuracy\"], alpha=0.5, label=\"acc\")\n",
        "# axs[1].plot(x, history.history[\"val_masked_accuracy\"], alpha=0.5, label=\"val_acc\")\n",
        "# axs[1].set_ylabel(\"Accuracy\")\n",
        "# axs[1].set_xlabel(\"epoch\")\n",
        "# axs[1].legend(loc=\"lower right\")\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PnJXGoeB-I4"
      },
      "source": [
        "**TEST**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NVkzw3JJjXcD"
      },
      "outputs": [],
      "source": [
        "# Load the trained model\n",
        "custom_objects = {\"PositionalEmbedding\": PositionalEmbedding,\n",
        "                  \"CustomSchedule\": CustomSchedule,\n",
        "                  \"masked_loss\": masked_loss,\n",
        "                  \"masked_accuracy\": masked_accuracy}\n",
        "with tf.keras.utils.custom_object_scope(custom_objects):\n",
        "    model = tf.keras.models.load_model(\"eng-fra-transformer.h5\")\n",
        "\n",
        "# training parameters used\n",
        "seq_len = 20\n",
        "vocab_size_en = 10000\n",
        "vocab_size_fr = 20000\n",
        "\n",
        "def translate(sentence):\n",
        "    \"\"\"Create the translated sentence\"\"\"\n",
        "    enc_tokens = eng_vectorizer([sentence])\n",
        "    lookup = list(fra_vectorizer.get_vocabulary())\n",
        "    start_sentinel, end_sentinel = \"[start]\", \"[end]\"\n",
        "    output_sentence = [start_sentinel]\n",
        "    # generate the translated sentence word by word\n",
        "    for i in range(seq_len):\n",
        "        vector = fra_vectorizer([\" \".join(output_sentence)])\n",
        "        assert vector.shape == (1, seq_len+1)\n",
        "        dec_tokens = vector[:, :-1]\n",
        "        assert dec_tokens.shape == (1, seq_len)\n",
        "        pred = model([enc_tokens, dec_tokens])\n",
        "        assert pred.shape == (1, seq_len, vocab_size_fr)\n",
        "        word = lookup[np.argmax(pred[0, i, :])]\n",
        "        output_sentence.append(word)\n",
        "        if word == end_sentinel:\n",
        "            break\n",
        "    return output_sentence\n",
        "\n",
        "test_count = 20\n",
        "for n in range(test_count):\n",
        "    english_sentence, french_sentence = random.choice(test_pairs)\n",
        "    translated = translate(english_sentence)\n",
        "    print(f\"Test {n}:\")\n",
        "    print(f\"{english_sentence}\")\n",
        "    print(f\"== {french_sentence}\")\n",
        "    print(f\"-> {' '.join(translated)}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UbY6jdP9lGNj"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}